{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ab3ed91",
   "metadata": {},
   "source": [
    "# Guess Core and use other methods to interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713dcc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pickle\n",
    "from scf_guess_tools import  Backend, load, calculate, guess\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import keras_tuner as kt\n",
    "import datetime\n",
    "import random, math\n",
    "import numpy as np\n",
    "from pyscf import scf\n",
    "from pyscf.lib.linalg_helper import eigh\n",
    "#! Only if there are no cudo GPUs in the system!\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "# os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"\n",
    "print(\"Built with CUDA support?\", tf.test.is_built_with_cuda())\n",
    "print(\"GPUs detected:\", tf.config.list_physical_devices(\"GPU\"))\n",
    "# List all physical GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(\"GPUs found:\", gpus)\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133c88d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from glob import glob\n",
    "sys.path.append(\"..\")\n",
    "from BlockMatrix import BlockMatrix\n",
    "from utils import  plot_mat_comp, flatten_triang_batch, flatten_triang, get_overlap, load_mol, unflatten_triang, density_from_fock, perform_calculation\n",
    "scripts_path = \"../../scripts\"\n",
    "if scripts_path not in sys.path:\n",
    "    sys.path.append(scripts_path)\n",
    "from to_cache import density_fock_overlap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b77b814",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_seed = 42\n",
    "source_path = '../../datasets/QM9/xyz_c7h10o2_sorted/'\n",
    "all_file_paths = glob(os.path.join(source_path, '*.xyz'))\n",
    "len(all_file_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813e44c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cached(file_paths, cache_path, basis, guess=\"minao\", method=\"dft\", functional=\"b3lypg\", backend=\"pyscf\", silent=True):\n",
    "    def sprint(*args): \n",
    "        if not silent:\n",
    "            print(*args)\n",
    "    error_list = []\n",
    "    error_files = []\n",
    "    focks = []\n",
    "    overlaps = []\n",
    "    used_files = []\n",
    "    reference_densities = []\n",
    "    for file in file_paths:\n",
    "        mol_name = os.path.basename(file).strip()\n",
    "        # sprint(mol_name)\n",
    "        try: \n",
    "            ret = density_fock_overlap(filepath = file,\n",
    "                                filename = mol_name,\n",
    "                                method = method,\n",
    "                                basis = None,\n",
    "                                functional = functional,\n",
    "                                guess = guess,\n",
    "                                backend = backend,\n",
    "                                cache = cache_path)\n",
    "            sprint(f\"Using: file={file} - mol_name={mol_name} - basis={None} - guess={guess} - method={method} - functional={functional}\")\n",
    "        except Exception as e: \n",
    "            error_list.append(e)\n",
    "            error_files.append(mol_name)\n",
    "            sprint(f\"File {mol_name} error - skipping\")\n",
    "            continue\n",
    "        if any([r == None for r in ret]): \n",
    "            sprint(f\"File {mol_name} bad - skipping\")\n",
    "            continue\n",
    "        focks.append(ret[1].numpy)\n",
    "        used_files.append(file)\n",
    "        reference_densities.append(ret[0].numpy)\n",
    "        overlaps.append(ret[2].numpy)\n",
    "    sprint(f\"Got data for: {len(focks)} - bad / no ret: {len(file_paths) - len(focks) - len(error_list)} - errors: {len(error_list)}\")\n",
    "    sprint(error_files[:5])\n",
    "    return focks, reference_densities, overlaps, used_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df905b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoundRobinLoader(Sequence):\n",
    "    def __init__(self, file_paths, cache_path, basis, guess, method,\n",
    "                 functional, backend, batch_size, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.file_paths  = list(file_paths)\n",
    "        self.cache_path  = cache_path\n",
    "        self.basis       = basis\n",
    "        self.guess       = guess\n",
    "        self.method      = method\n",
    "        self.functional  = functional\n",
    "        self.backend     = backend\n",
    "        self.batch_size  = batch_size\n",
    "        self.indexes     = list(range(len(self.file_paths)))\n",
    "        random.shuffle(self.indexes)\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.file_paths) / self.batch_size)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_idxs  = self.indexes[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "        batch_files = [self.file_paths[i] for i in batch_idxs]\n",
    "\n",
    "        # load a small batch into memory\n",
    "        focks, dens, ovlps, _ = load_cached(\n",
    "            batch_files, self.cache_path, self.basis,\n",
    "            self.guess, self.method, self.functional, self.backend\n",
    "        )\n",
    "\n",
    "        X = np.array([flatten_triang(ov) for ov in ovlps])\n",
    "        # y = np.array([flatten_triang(fk) for fk in focks])# full fock as target!\n",
    "        y = np.array([np.diag(fk) for fk in focks])\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        random.shuffle(self.indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f2fcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ret = load_cached(all_file_paths, \"../../datasets/QM9/out/c7h10o2_b3lypg_6-31G(2df,p)_sorted/pyscf\", basis=\"6-31g_2df_p_custom_nwchem.gbs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b16f25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_test_seed = 42\n",
    "cache_path = \"../../datasets/QM9/out/c7h10o2_b3lypg_6-31G(2df,p)_sorted/pyscf\"\n",
    "basis = \"../../scripts/6-31g_2df_p_custom_nwchem.gbs\"\n",
    "train_files, val_files = train_test_split(all_file_paths, test_size=0.2, random_state=train_test_seed)\n",
    "val_files, test_files = val_files[:len(val_files)//2], val_files[len(val_files)//2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5329ece4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = RoundRobinLoader(\n",
    "    file_paths=train_files,\n",
    "    cache_path=cache_path,\n",
    "    basis=basis,\n",
    "    guess=\"minao\",\n",
    "    method=\"dft\",\n",
    "    functional=\"b3lypg\",\n",
    "    backend=\"pyscf\",\n",
    "    batch_size=32\n",
    ")\n",
    "val_loader = RoundRobinLoader(\n",
    "    file_paths=val_files,\n",
    "    cache_path=cache_path,\n",
    "    basis=basis,\n",
    "    guess=\"minao\",\n",
    "    method=\"dft\",\n",
    "    functional=\"b3lypg\",\n",
    "    backend=\"pyscf\",\n",
    "    batch_size=32\n",
    ")\n",
    "test_loader = RoundRobinLoader(\n",
    "    file_paths=test_files,\n",
    "    cache_path=cache_path,\n",
    "    basis=basis,\n",
    "    guess=\"minao\",\n",
    "    method=\"dft\",\n",
    "    functional=\"b3lypg\",\n",
    "    backend=\"pyscf\",\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2413166a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_dim = train_loader[0][0].shape\n",
    "mat_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b1c70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data stats: \n",
    "train_mean = np.mean(train_loader[0][0])\n",
    "train_std = np.std(train_loader[0][0])\n",
    "print(\"Train mean:\", train_mean)\n",
    "print(\"Train std:\", train_std)\n",
    "print(\"Train max\", np.max(train_loader[0][0]))\n",
    "print(\"Train min\", np.min(train_loader[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e784c71b",
   "metadata": {},
   "source": [
    "let's try our luck! - no rescaling for now "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be959e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "X_sample, y_sample = train_loader[0]\n",
    "flattened_dim = X_sample.shape[1]\n",
    "diag_dim      = y_sample.shape[1]\n",
    "print(diag_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6749b619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hp_depth(hp):\n",
    "    inputs_orig = tf.keras.Input(shape=(flattened_dim,))\n",
    "    x = inputs_orig\n",
    "    for i in range(hp.Int(\"n_layers\", 1,4)):\n",
    "        neurons = 512\n",
    "        dropout_rate = 0.1\n",
    "        x = tf.keras.layers.Dense(neurons, activation='gelu')(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "    outputs = tf.keras.layers.Dense(diag_dim)(x)\n",
    "\n",
    "    basic_model = tf.keras.Model(inputs=inputs_orig, outputs=outputs)\n",
    "\n",
    "    initial_lr = 0.01\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=initial_lr,\n",
    "        decay_steps=100,\n",
    "        decay_rate=0.96,\n",
    "        staircase=False\n",
    "    )\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "    basic_model.compile(optimizer=optimizer, loss=\"mae\", metrics=[\"mse\"])\n",
    "    return basic_model\n",
    "\n",
    "def hp_model(hp): \n",
    "    tf.random.set_seed(42)\n",
    "    inputs_orig = tf.keras.Input(shape=(flattened_dim,))\n",
    "    x = inputs_orig\n",
    "    for i in range(hp.Int(\"n_layers\", 1,4)):\n",
    "        neurons = hp.Choice(f\"neuron_count_{i}\", [512,1024])\n",
    "        dropout_rate = hp.Float(f\"dropout_rate_{i}\", 0.0, 0.2, step=0.05)\n",
    "        x = tf.keras.layers.Dense(neurons, activation='gelu')(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "    outputs = tf.keras.layers.Dense(diag_dim)(x)\n",
    "\n",
    "    basic_model = tf.keras.Model(inputs=inputs_orig, outputs=outputs)\n",
    "\n",
    "    initial_lr = 0.01\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=initial_lr,\n",
    "        decay_steps=100,\n",
    "        decay_rate=0.96,\n",
    "        staircase=False\n",
    "    )\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "    basic_model.compile(optimizer=optimizer, loss=\"mae\", metrics=[\"mse\"])\n",
    "    return basic_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ffcccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClearMemory(tf.keras.callbacks.Callback):\n",
    "    def on_train_end(self, logs=None):\n",
    "        tf.keras.backend.clear_session()\n",
    "        import gc; gc.collect()\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',      \n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\"models/basic_model_6_31G.keras\", save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b92725",
   "metadata": {},
   "source": [
    "# First search optimal depth!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666c491e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_depth = kt.Hyperband(\n",
    "    hp_depth, \n",
    "    objective='val_loss',\n",
    "    max_epochs=30,\n",
    "    factor=3,\n",
    "    initial_epoch=0,\n",
    "    directory='keras_tuning',\n",
    "    project_name='fock_diag_nn_631_tuning_depth'\n",
    ")\n",
    "tuner_depth.search(\n",
    "    train_loader,\n",
    "    validation_data=val_loader,\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    callbacks=[tensorboard_callback, checkpoint_callback, early_stop, ClearMemory()],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5d8c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_depth.results_summary(num_trials=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ae49d6",
   "metadata": {},
   "source": [
    "# Search optimal other params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2c06b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(\n",
    "    hp_model,\n",
    "    objective=\"val_mse\",\n",
    "    max_epochs=30,\n",
    "    factor=3,\n",
    "    directory=\"keras_tuning\",\n",
    "    project_name=\"fock_diag_nn_631_tuning\"\n",
    ")\n",
    "tuner.search(\n",
    "    train_loader,\n",
    "    validation_data=val_loader,\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    steps_per_epoch=16,\n",
    "    validation_steps=4,\n",
    "    callbacks=[tensorboard_callback, checkpoint_callback, early_stop, ClearMemory()],\n",
    "    verbose=1\n",
    ")\n",
    "# basic_hist = basic_model.fit(train_loader,\n",
    "#                             validation_data=val_loader,\n",
    "#                             epochs=50,\n",
    "#                             batch_size=32,\n",
    "#                             callbacks=[tensorboard_callback, checkpoint_callback, early_stop],\n",
    "#                             verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae6d433",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial = tuner.oracle.get_best_trials(num_trials=1)[0]\n",
    "hyperparams = best_trial.hyperparameters.values\n",
    "print(\"Best hyperparameters:\")\n",
    "for key, value in hyperparams.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "val_loss_history = best_trial.metrics.get_history(\"val_loss\")\n",
    "loss_history = best_trial.metrics.get_history(\"loss\")\n",
    "\n",
    "fresh_model = tuner.hypermodel.build(best_trial.hyperparameters)\n",
    "# Get all Dropout rates in fresh_model\n",
    "dropout_rates = [layer.rate for layer in fresh_model.layers if isinstance(layer, tf.keras.layers.Dropout)]\n",
    "print(\"Dropout rates in fresh_model:\", dropout_rates)\n",
    "\n",
    "fresh_model.summary()\n",
    "\n",
    "\n",
    "history = fresh_model.fit(\n",
    "    train_loader,\n",
    "    validation_data=test_loader, # validate on test set!\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[tensorboard_callback, checkpoint_callback, early_stop, ClearMemory()],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee837c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation loss\n",
    "basic_model = fresh_model\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "plt.plot(train_loss, label='train')\n",
    "plt.plot(val_loss, label='validation')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('mae')\n",
    "plt.title('Model loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df4df3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"models\", exist_ok=True)\n",
    "if os.path.exists(\"models/basic_model_6_31G.keras\") and input(\"Overwrite existing model? (y/n)\") == \"y\":\n",
    "    os.remove(\"models/basic_model_6_31G.keras\")\n",
    "    # also save history\n",
    "    with open(\"models/basic_model_6_31G_history.pkl\", \"wb\") as f:\n",
    "        pickle.dump(history.history, f)\n",
    "    basic_model.save(\"models/basic_model_6_31G.keras\")\n",
    "    print(\"Model saved\")\n",
    "else:\n",
    "    basic_model.save(\"models/basic_model_6_31G.keras\")\n",
    "    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314eab9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "loaded_model = tf.keras.models.load_model(\"models/basic_model_6_31G.keras\")\n",
    "basic_model = loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3af4898",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_Fock(diag, ovlp, K = 1.75): \n",
    "    \"\"\"Take diagonal and reconstruct the Fock matrix using GWH\n",
    "    \"\"\"\n",
    "    mat_dim = diag.shape[0]\n",
    "    out = np.zeros((mat_dim, mat_dim))\n",
    "    for i in range(mat_dim):\n",
    "        for j in range(mat_dim):\n",
    "            if i == j:\n",
    "                out[i, j] = diag[i]\n",
    "            else:\n",
    "                out[i, j] = K * ovlp[i, j] * (diag[i] + diag[j]) / 2\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4384ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict: \n",
    "test_pred_fock_diag = basic_model.predict(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52dda67",
   "metadata": {},
   "source": [
    "# Comparison diag error with minao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efe39a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fock_minao_guess(mol):\n",
    "    mf = scf.RHF(mol)\n",
    "    D0 = mf.get_init_guess(mol, key='minao')\n",
    "    return mf.get_fock(dm=D0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d282b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_minao, rmse_pred = [], []\n",
    "for i, (file, pred) in enumerate(zip(test_loader.file_paths, test_pred_fock_diag)):\n",
    "    try:\n",
    "        test_mol = load(file, symmetry=False, basis = basis, backend=Backend.PY)\n",
    "        ref_diag = test_loader[i//32][1][i % 32]\n",
    "        minao_pred_fock = np.diag(get_fock_minao_guess(test_mol.native))\n",
    "        rmse_minao.append(np.sqrt(np.mean((minao_pred_fock - ref_diag) ** 2)))\n",
    "        rmse_pred.append(np.sqrt(np.mean((pred - ref_diag) ** 2)))\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file}: {e}\")\n",
    "        continue\n",
    "    rmse_minao_mean, rmse_pred_mean = np.mean(rmse_minao), np.mean(rmse_pred)\n",
    "    rmse_minao_std, rmse_pred_std = np.std(rmse_minao), np.std(rmse_pred)\n",
    "    print(f\"RMSE MINAO: {rmse_minao_mean:.4f} ± {rmse_minao_std:.4f}\")\n",
    "    print(f\"RMSE Pred: {rmse_pred_mean:.4f} ± {rmse_pred_std:.4f}\")\n",
    "    print(i)\n",
    "    if i > 50: \n",
    "        break\n",
    "\n",
    "rmse_minao_mean, rmse_pred_mean = np.mean(rmse_minao), np.mean(rmse_pred)\n",
    "rmse_minao_std, rmse_pred_std = np.std(rmse_minao), np.std(rmse_pred)\n",
    "print(f\"RMSE MINAO: {rmse_minao_mean:.4f} ± {rmse_minao_std:.4f}\")\n",
    "print(f\"RMSE Pred: {rmse_pred_mean:.4f} ± {rmse_pred_std:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ce8f28",
   "metadata": {},
   "source": [
    "# Full matrix comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d295d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example comparison: \n",
    "rand_test_sample = np.random.randint(0, len(test_loader.file_paths) - 1)\n",
    "# compare with hückel and minao\n",
    "file_path = test_loader.file_paths[rand_test_sample]\n",
    "print(basis)\n",
    "test_mol = load(file_path, symmetry=False, basis = basis, backend=Backend.PY)\n",
    "focks_ref, dens_ref, ovlps_ref, _ = load_cached(\n",
    "    [file_path],\n",
    "    cache_path,\n",
    "    None,\n",
    ")\n",
    "ground_truth_fock    = focks_ref[0]\n",
    "ground_truth_density = dens_ref[0]\n",
    "S_test_mol                    = ovlps_ref[0]\n",
    "\n",
    "minao_guess = guess(test_mol, method=\"dft\", basis=basis, scheme=\"minao\")\n",
    "hueckel_guess = guess(test_mol, method=\"dft\", basis=basis, scheme=\"huckel\")\n",
    "\n",
    "pred_flat_diag = test_pred_fock_diag[rand_test_sample]\n",
    "pred_fock      = reconstruct_Fock(pred_flat_diag, S_test_mol)\n",
    "\n",
    "nelec = test_mol.native.nelectron//2\n",
    "pred_density_example = density_from_fock(pred_fock, S_test_mol, nelec)\n",
    "\n",
    "plot_mat_comp(ground_truth_fock, pred_fock, title=\"Fock matrix prediction Basic NN\", vmax=0.5)\n",
    "plot_mat_comp(ground_truth_fock, minao_guess.fock().numpy, title=\"Fock matrix prediction MINAO\", vmax=0.5)\n",
    "plot_mat_comp(ground_truth_fock, hueckel_guess.fock().numpy, title=\"Fock matrix prediction Hückel\", vmax=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487dac62",
   "metadata": {},
   "outputs": [],
   "source": [
    "density_from_fock_minao = density_from_fock(minao_guess.fock().numpy, S_test_mol, test_mol.native.nelectron//2)\n",
    "plot_mat_comp(ground_truth_density, pred_density_example, title=\"Density matrix prediction Basic NN\", vmax=0.1)\n",
    "plot_mat_comp(ground_truth_density, density_from_fock_minao, title=\"Density matrix prediction MINAO - from fock\", vmax=0.1)\n",
    "plot_mat_comp(ground_truth_density, minao_guess.density().numpy, title=\"Density matrix prediction MINAO\", vmax=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d39f0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test sim\n",
    "minao_res = calculate(test_mol, method=\"dft\", functional=\"b3lypg\", basis=basis, guess=\"minao\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e75836",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_res = perform_calculation(file_path, pred_density_example, method=\"dft\", functional=\"b3lypg\", basis_set=basis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2752e7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nn_res[\"cycles\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be91e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(minao_res.native.cycles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dc7e67",
   "metadata": {},
   "source": [
    "# Test one iteration SCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9868c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scf_one_iteration_from_minao(mol, D0=None):\n",
    "    \"\"\"\n",
    "    Perform one SCF iteration starting from a MINAO guess.\n",
    "    Returns the new density matrix D1 and the total energy E_tot.\n",
    "    \"\"\"\n",
    "    mf = scf.RHF(mol)\n",
    "    # 1) Get integrals and overlap\n",
    "    S    = mf.get_ovlp()\n",
    "    hcore = mf.get_hcore()\n",
    "    e_nuc = mol.energy_nuc()\n",
    "    if D0 is None: \n",
    "        # 2) Build MINAO guess density\n",
    "        D0 = mf.get_init_guess(mol, key='minao')\n",
    "    # 3) Build Fock from D0\n",
    "    F0 = mf.get_fock(dm=D0)\n",
    "\n",
    "    nocc   = mol.nelectron // 2\n",
    "    D1 = density_from_fock(F0, S, nocc)\n",
    "    # 7) Compute SCF energy: E_el = ½ Tr[D1 (hcore+F0)], E_tot = E_el + E_nuc\n",
    "    e_el   = 0.5 * np.einsum('ij,ij', D1, hcore + F0)\n",
    "    E_tot  = e_el + e_nuc\n",
    "\n",
    "    return D1, E_tot, S, hcore, F0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba62c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "d, e, S, hcore, F = scf_one_iteration_from_minao(test_mol.native)\n",
    "plot_mat_comp(ground_truth_density, d, title=\"Density matrix prediction MINAO - from fock\", vmax=0.1)\n",
    "plot_mat_comp(minao_guess.fock().numpy, F, title=\"minao_fock vs minao_fock\", vmax=0.1)\n",
    "plot_mat_comp(minao_guess.overlap().numpy, S, title=\"minao_overlap vs minao_overlap\", vmax=0.1)\n",
    "density_from_minao_fock = density_from_fock(minao_guess.fock().numpy, minao_guess.overlap().numpy, test_mol.native.nelectron//2)\n",
    "plot_mat_comp(ground_truth_density, density_from_minao_fock, title=\"truth density vs. minao from fock density\", vmax=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3184dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scf_guess_1_0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
