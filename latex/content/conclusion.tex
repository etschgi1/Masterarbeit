\chapter{Conclusion \& Outlook}
\label{chap:conclusion}
This thesis presents a novel approach to density matrix prediction of molecular systems using machine learning techniques. 
Initially, an indirect way of predicting the density matrix from the overlap was explored, which involved predicting the Fock matrix and subsequently deriving the electron density in a given atom-centred basis set. Ridge- and Kernel Ridge Regression methods performed well against established schemes when employing a minimal basis set. However, moving to larger basis sets, small deviations in the Fock matrix led to enormous errors in the density matrix. Linear models, as well as a simple MLP network, failed to deliver adequate Fock predictions to derive accurate density matrices.\\
To mitigate these issues, a message passing Graph Neural Network (GNN) was designed in \autoref{chap:gnn}, to directly predict the sparse density matrix from the overlap matrix. A flexible per-atom-type encoder/decoder architecture was implemented, which allows for a handling of molecular systems of varying size. Furthermore, invariance to molecular rotation was ensured by augmenting the training data with rotated versions of the molecules. \\
In \autoref{chap:application}, the GNN was tested on three distinct datasets and achieved iteration-to-convergence performance on par with the best classical guessing schemes.\\% Remarkably, despite its poorer self-consistency and larger energy errors, it still requires no more SCF iterations than all but one established methods.\\

The primary limitation for all learning models is their reliance on a surrogate loss (mean squared error of the density matrix prediction) to minimize iterations. Training directly on the iteration count would likely boost network performance, but at the expense of immense computational effort. Moreover, the GNN restricts message passing to atoms, so matrix elements representing interatomic interaction (interaction blocks) are passively mapped via encoder/decoder and do not engage in message passing. While initial experimentation using updaters during message passing for interaction blocks showed promising results, it was not pursued further due to the larger complexity of the model.\\

Various avenues for future work are suggested, including further development of the GNN architecture, extension to larger and more varied systems, and the incorporation of a novel loss function to better align the model with the SCF iteration count. \\
Improvements of the architecture could include the addition of the aforementioned interaction block updaters, as well as additional embeddings to better capture the chemical environment. Implementing equivariant capabilities intrinsically in the GNN would eliminate the need for data augmentation, which did not improve network performance on our isomer dataset. Consequently, such an architectural change is advantageous only for systems that do not already offer a high diversity in molecule orientations.\\
While excellent prediction capabilities were observed for organic compounds including \ch{H}, \ch{C}, \ch{O}, \ch{N} and \ch{F}, the model's performance on transition metals and larger systems remains to be explored. Promising results for the MD-trajectory dataset suggest that the application to transition state geometries might be feasible.
Furthermore, application on hard to convergence transition metal complexes and heavy-elements with strong relativistic effects could be a worthwhile endeavour. \\
Finally, a hybrid loss function that combines RMSE with energy- and/or DIIS error might be promising. To avoid the costly derivation of the Fock matrix, crucial for these metrics, one might make use of twin networks, where one network predicts the density matrix and the other the Fock matrix. Combining the RMSE from one network with the DIIS error given by their predictions\footnote{DIIS may be calculated using the density guess of one network and the Fock guess of the other (see \autoref{eq:diis_error}).}, one could simultaneously minimize DIIS and RMSE without directly training on iteration count.
