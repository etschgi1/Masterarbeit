\chapter{Conclusion \& Outlook}
\label{chap:conclusion}
This thesis presents a novel approach to predict the density matrix of molecular systems using machine learning techniques. 
Initially, an indirect way of predicting the density matrix from the overlap was explored, which involved predicting the Fock matrix and subsequently deriving the density. Ridge- and Kernel Ridge Regression methods fared well against established schemes for a minimal basis set. However, moving to larger basis sets, small deviations in the Fock matrix led to enormous errors in the density matrix. Linear models, as well as a simple MLP network, failed to deliver adequate Fock predictions to derive accurate density matrices.\\

To mitigate these issues, a Graph Neural Network (GNN) was designed in \autoref{chap:gnn} to directly predict the sparse density matrix from the overlap matrix. A flexible per atom-type encoder / decoder architecture was implemented, which allows for the handling of different molecule sizes. Furthermore, invariance to molecule rotations was ensured by augmenting the training data with rotated versions of the molecules. \\
In \autoref{chap:application}, the GNN was tested on three distinct datasets and achieved iteration-to-convergence performance on par with the best classical guessing schemes. Remarkably, despite its poorer self-consistency and larger energy errors, it still requires no more SCF iterations than all but one established methods.\\

The primary limitation for all learning models is their reliance on a surrogate loss (mean squared error of the density matrix prediction) to minimize iterations. Training directly on the iteration count would likely boost the network's performance but at the expense of immense computational effort. Moreover, the GNN restricts message passing to atoms, so matrix elements representing interatomic interaction (interaction blocks) are passively mapped via encoder/decoder and do not engage in message passing. While initial experimentation using updaters during message passing for interaction blocks showed promising results, it was not pursued further due to the higher complexity of the model.\\

Various avenues for future work are suggested, including further development of the GNN architecture, extension to larger and more varied systems and incorporation of a novel loss function to better align the model with the SCF iteration count. \\
Improvements of the architecture could include the addition of the aforementioned interaction block updaters, as well as additional embeddings to better capture the chemical environment. Implementing equivariant capabilities intrinsically in the GNN would make data augmentation, which didn't improve the network's performance on our isomer dataset, obsolete. Consequently, this architectural change only benefits systems that aren't already inherently diverse.  \\
While excellent prediction capabilities were shown for (mainly) organic compounds, including: \ch{H}, \ch{C}, \ch{O}, \ch{N} and \ch{F}, the model's performance on transition metals and larger systems remains to be explored. Promising results for the MD-trajectory dataset suggest that the application to transition state geometries might be feasible.
Furthermore, application too hard to convergence transition metal complexes as well as heavy-elements with strong relativistic effects could be a worthwhile endeavor. \\
Finally, a hybrid loss function that combines RMSE with energy- and/or DIIS error can be developed. To avoid the costly derivation of the Fock matrix, crucial for these metrics, one might make use of twin networks, where one network predicts the density matrix and the other the Fock matrix. By combining the RMSE from one network with the DIIS error, given by the combined predictions, we'd simultaneously minimize DIIS and RMSE without directly training on iteration count.
