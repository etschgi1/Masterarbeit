\chapter{Application}
\label{chap:application}
The GNN model introduced in \autoref{chap:gnn} will be benchmarked on various datasets in the following chapters. All reference data was calculated using the 6-31G(2df,p) basis at theory level B3LYP. \\

Models will be evaluated by their iteration count till convergence, the energy difference from the converged solution and the DIIS error (see \autoref{eq:diis_error}). 

\TODO{Refer to 0d benchmark model, check basis + functional}
\section{QM9 - \ch{C7H10O2} Isomers}
\label{sec:qm9_isomers_benchmark}
There are 6095 structural isomers of \ch{C7H10O2} in the QM9 dataset (see \autoref{subsec:qm9}). Analogous to the trials performed in \autoref{sec:further_trials_mlp}, we will train and validate on a randomly drawn sample of 500 isomers\footnote{using \textsc{scf\_guess\_datasets} (see \autoref{subsec:gnn_normalization})}. This reduction is necessary to make training and hyperparameter-tuning feasible in the scope of the thesis. Contrary to the full matrix prediction schemes in \autoref{chap:fock_matrix_predictions}, we employ sub-matrix predictions and reconstruct the full matrix thus making the actual number of samples significantly higher. Per molecule sample we get 7, 10 and 2 samples for \ch{C}, \ch{H} and \ch{O} respectively, totalling 3500 \ch{C}, 5000 \ch{H} and 1000 \ch{O} samples. This already provides some rotational variability. Additional rotations can be introduced through data augmentation during training to develop a model that is agnostic to rotation when predicting density.\\

%! refer to MGNN_6-31G_NO_AUG_07_07_manual_ref.pth
\subsection{Initial training}
\label{subsec:qm9_isomers_initial}
To gauge the performance of the GNN devised in \autoref{chap:gnn} manual runs were performed during development. Hyperparameters were set according to \autoref{tab:init_hparams}. 

\begin{table}[H]
    \centering
    \caption{Hyperparameters used for the initial MGNN training (manually selected)}
    \label{tab:init_hparams}
    \begin{tabular}{ll ll}
        \toprule
        \textbf{Hyperparameter} & \textbf{Value} & \textbf{Hyperparameter} & \textbf{Value} \\
        \midrule
        Hidden dimension & 256 & Msg. passing rounds & 4 \\
        MsgNet layers & 3 & MsgNet dropout & 15 \% \\
        Batch size & 16 & Grace period & 10 epochs \\
        Target & Density matrix & Loss function & MSE (block-wise) \\
        Learn rate (initial) & $2.68 \times 10^{-3}$ & Weight decay & $1.78 \times 10^{-5}$ \\
        Edge threshold & 3 \AA & Data augmentation & No \\
        \midrule
        Learn rate factor & 0.5 & Learn rate patience & 3 epochs \\
        Learn rate threshold & $10^{-3}$ & Learn rate cooldown & 2 epochs \\
        Learn rate min & $10^{-6}$ & — & — \\
        \bottomrule
    \end{tabular}
\end{table}
Note, that this initial run did not use data augmentation and thus trained on 400 samples (corresponding to our default datasplit of 80\% / 10\% / 10\%). The grace period, time without improvement, was set to 10 epochs to allow the learning rate scheduler sufficient time to take effect and for potential improvements to manifest thereafter.\\
Training and validation losses both monotonically decrease up to around epoch 30 as can be seen in \autoref{fig:initial_train_qm9_isomers}. While the loss on the validation set plateaus rather early, training loss decreases throughout the training process. 

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{../fig/gnn/MGNN_6-31G_NO_AUG_07_07_manual_ref_train_val_loss.pdf}
    \caption[Initial GNN loss on QM9-isomers]{Initial GNN training / validation loss and corresponding learn rate per epoch on QM9-isomers.}
    \label{fig:initial_train_qm9_isomers}
\end{figure}
Both losses are further pushed down following learn rate decreases. This run produced the best model in epoch 33 with a validation loss of $33.00$ and a training loss of $28.83$ indicating slight overfitting which is to be expected especially without data augmentation in the training samples. The performance of the model $\text{GNN}_\text{initial}$ on the test set is compared to other models and guessing schemes in \autoref{tab:qm9_isomers_test_overview}. 



\subsection{Hyperparameter tuning}
\label{subsec:qm9_isomers_hyperparamtuning}
Generally one uses the validation loss as a benchmark to select the best model from a hyperparameter run. While we will also prefer models with lower loss, we have to be very careful not to select models which look good on paper but perform worse due to the lack good correlation between MSE and iteration count. For this reason we will base our hyperparameter search on the $\text{GNN}_\text{initial}$ model and explore in a structured way. 

\textbf{Data augmentation}\\
$\text{GNN}_\text{initial}$ already performed quite well in terms of iterations without using any data augmentation. One might argue that there is already some data augmentation baked into the train set due to the different orientations of atoms in various molecules. 
\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\textwidth]{../fig/application/aug_train_val_loss.pdf}
    \caption[GNN loss different augmentation factors | QM9-isomers]{GNN loss for different data augmentation factors on QM9-isomers. All other hyperparameters are kept as in \autoref{tab:init_hparams}.}
    \label{fig:loss_hyper_qm9_isomers}
\end{figure}
Comparing the training and validation loss between different augmentation factors in \autoref{fig:loss_hyper_qm9_isomers} shows no clear trend regarding the choice of the augmentation factor. While a factor of $2.5$ initially outperforms no augmentation and other factors, they all converge in validation towards the end. 
Tuning the 

\begin{table}[H]
    \centering
    \caption{Different data augmentation}
    \label{tab:qm9_isomers_data_aug_hyperparam}
    \resizebox{\textwidth}{!}{
        \begin{tabular}{l
                        S[table-format=2.1(2)]
                        S[table-format=-4(4)]
                        S[table-format=-1.3(2)]
                        S[table-format=1.3(1.1)]
                        S[table-format=1.4(1)]}
            \toprule
            Mean metrics:                 & {Iterations / 1} & {$\Delta E_\text{HF}$ / $\unit{\hartree}$}  & {$\delta E_\text{HF}$ / 1} & {DIIS error / $\unit{\hartree}$} & {$\overline{\text{RMSE}}$ / $\unit{\hartree}$} \\
            \midrule
            $\text{GNN}_\text{initial}$  & \textbf{11.2(5)}  & \TODO{ENERGY with new cached}  &  & \textbf{0.17(2)}  & \textbf{0.0078(6)}\\
            $\text{GNN}_\text{aug. 1.5}$ & 11.2(6)  &   &  & 0.205(15)& \textbf{0.0078(6)}\\
            $\text{GNN}_\text{aug. 2.0}$ & 11.5(6)  &   &  & 0.209(17)& 0.0079(6)\\
            $\text{GNN}_\text{aug. 2.5}$ & 12.0(7)  &   &  & 0.216(18)& 0.0081(6)\\
            $\text{GNN}_\text{aug. 3.0}$ & 11.4(8)  &   &  & 0.206(16)& 0.0081(6)\\
            $\text{GNN}_\text{aug. 3.5}$ & 11.3(6)  &   &  & 0.204(17)& 0.0080(6)\\
            $\text{GNN}_\text{aug. 4.0}$ & 11.6(10) &   &  & 0.213(16)& 0.0080(6)\\
            \bottomrule
        \end{tabular}
    }
\end{table}

\TODO{RESULTS FROM HYPERPARAMETER OPTIMIZATION RUNS - EVAL}\\

\textbf{Note to self: table 5.2 and 5.3 were computed without global loss (meaning loss was evaluated only on centerblocks + blocks which are within threshold!) -> Another manual run with global loss should be done maybe for Hyperparam optimization runs! -> then check if that performs better! }

\textbf{Investigate edge\_threshold}\\
\begin{table}[H]
    \centering
    \caption{Different edge formation thresholds}
    \label{tab:qm9_isomers_dist_hyperparam}
    \resizebox{\textwidth}{!}{
        \begin{tabular}{l
                        S[table-format=2.1(2)]
                        S[table-format=-4(4)]
                        S[table-format=-1.3(2)]
                        S[table-format=1.3(1.1)]
                        S[table-format=1.4(1)]}
            \toprule
            Mean metrics:                 & {Iterations / 1} & {$\Delta E_\text{HF}$ / $\unit{\hartree}$}  & {$\delta E_\text{HF}$ / 1} & {DIIS error / $\unit{\hartree}$} & {$\overline{\text{RMSE}}$ / $\unit{\hartree}$} \\
            \midrule
            $\text{GNN}_\text{dist. thres. 1.5}$ & 15.1(19)&   &  & \textbf{0.184(15)}& 0.0104(5)\\
            $\text{GNN}_\text{dist. thres. 2.0}$ & 12.3(9) &   &  & 0.226(18)& 0.0084(5)\\
            $\text{GNN}_\text{dist. thres. 2.5}$ & \textbf{11.2(5)} &   &  & 0.205(15)& 0.0080(6)\\
            $\text{GNN}_\text{dist. thres. 3.0}$ & 11.8(13)& \TODO{ENERGY with new cached}  &  & 0.208(16)  & 0.0079(6)\\
            $\text{GNN}_\text{dist. thres. 3.5}$ & 12.6(8) &   &  & 0.210(16)& 0.0079(6)\\
            $\text{GNN}_\text{dist. thres. 4.0}$ & 12.0(9) &   &  & 0.202(16)& 0.0079(6)\\
            $\text{GNN}_\text{dist. thres. 4.5}$ & 12.4(10)&   &  & 0.206(15)& 0.0080(6)\\
            $\text{GNN}_\text{dist. thres. 5.0}$ & 11.2(7) &   &  & 0.197(14)& \textbf{0.0077(7)}\\
            \bottomrule
        \end{tabular}
    }
\end{table}

--> compare initial vs. dist. thres. 3.0
\begin{verbatim}
    
    {
        "batch_size": 16,
        "data_aug_factor": 1,
        "edge_threshold_val": 3.0,
        "grace_epochs": 5,
        "hidden_dim": 256,
        "lr": 0.00268,
        "lr_cooldown": 2,
        "lr_factor": 0.5,
        "lr_min": 1e-06,
        "lr_patience": 3,
        "lr_threshold": 0.001,
        "message_net_dropout": 0.15,
        "message_net_layers": 3,
        "message_passing_steps": 4,
        "num_epochs": 50,
        "weight_decay": 1.78e-05
        }
\end{verbatim}
\textbf{Further hyperparameter optimization runs}\\

\begin{table}[H]
    \centering
    \caption{Further runs}
    \label{tab:qm9_isomers_further_runs}
    \resizebox{\textwidth}{!}{
        \begin{tabular}{l
                        S[table-format=2.1(2)]
                        S[table-format=-4(2)]
                        S[table-format=-1.3(2)]
                        S[table-format=1.3(1)]
                        S[table-format=1.4(1)]}
            \toprule
            Mean metrics:                 & {Iterations / 1} & {$\Delta E_\text{HF}$ / $\unit{\hartree}$}  & {$\delta E_\text{HF}$ / 1} & {DIIS error / $\unit{\hartree}$} & {$\overline{\text{RMSE}}$ / $\unit{\hartree}$} \\
            \midrule
            $\text{GNN}_\text{hyp. 0}$ & \textbf{11.5(9)} & -1250(40)          & -0.718(0.011)          & 0.209(0.02) & \textbf{0.0076(6)} \\
            $\text{GNN}_\text{hyp. 1}$ & 11.6(0.7)        & -1250(40)          & -0.713(0.012)          & 0.206(0.02) & 0.0078(0.0006) \\
            $\text{GNN}_\text{hyp. 2}$ & 11.7(0.8)        & \textbf{-1190(40)} & \textbf{-0.680(10)} & \textbf{0.190(2)} & 0.0079(0.0006) \\
            $\text{GNN}_\text{hyp. 3}$ & 13.1(1.1)        & -1310(50)          & -0.752(0.014) & 0.230(0.02) & 0.0084(0.0005) \\
            $\text{GNN}_\text{hyp. 4}$ & 13.2(1.2)        & -1310(40)          & -0.752(0.012) & 0.234(0.02) & 0.0088(0.0005) \\
            $\text{GNN}_\text{hyp. 5}$ & 13.3(1.1)        & -1300(40)          & -0.743(0.012) & 0.232(0.02) & 0.0090(0.0005) \\
            $\text{GNN}_\text{hyp. 6}$ & 13.6(1.3)        & -1320(40)          & -0.753(0.014) & 0.235(0.02) & 0.0089(0.0005) \\
            $\text{GNN}_\text{hyp. 7}$ & 13.9(1.1)        & -1320(50)          & -0.758(0.014) & 0.238(0.02) & 0.0088(0.0006) \\
            $\text{GNN}_\text{hyp. 8}$ & 14.5(1.4)        & -1350(40)          & -0.770(0.012) & 0.244(0.02) & 0.0090(0.0005) \\
            $\text{GNN}_\text{hyp. 9}$ & 14.7(1.2)        & -1350(50)          & -0.772(0.016) & 0.248(0.02) & 0.0091(0.0005) \\
            \bottomrule
        \end{tabular}
    }
\end{table}
\TODO{write something about very well performing RMSE runs}


\subsection{Evaluation \& Conclusion}
\label{subsec:qm9_isomers_eval_and_concl}
The models defined above are compared to PySCF guessing schemes in \autoref{tab:qm9_isomers_test_overview}
\begin{table}[H]
    \centering
    \caption{Comparison of different models with PySCF guessing schemes.}
    \label{tab:qm9_isomers_test_overview}
    \resizebox{\textwidth}{!}{
        \begin{tabular}{l
                        S[table-format=2.1(2)]
                        S[table-format=-4(4)]
                        S[table-format=-1.3(2)]
                        S[table-format=2.1(1.1)]
                        S[table-format=1.4(1)]}
            \toprule
            Mean metrics:                 & {Iterations / 1} & {$\Delta E_\text{HF}$ / $\unit{\hartree}$}  & {$\delta E_\text{HF}$ / 1} & {DIIS error / $\unit{\hartree}$} & {$\overline{\text{RMSE}}$ / $\unit{\hartree}$} \\
            \midrule
            $\text{GNN}_\text{initial}$   & 11.2(5)  & 9000(9300)  & 5.8(60)   & 0.17(2)  & 0.0078(6)\\
            $\text{GNN}_\text{hyp. best}$ &          &             &           &          &          \\
            0-D                           & 17.1(15) & -26(13)     & -0.017(8) & 0.01(0)  & 0.0138(4)\\
            1-e                           & 18.8(18) & -8000(100)  & -5.087(13)& 0.51(6)  & 0.14(4)  \\
            vsap                          & 14.2(9)  & -8500(4)    & -5.45(5)  & 1.15(10) & 0.0109(7)\\
            atom                          & 16.6(19) & -8600(30)   & -5.51(6)  & 1.12(10) & 0.016(2) \\
            minao                         & 10.8(6)  & -8122(3)    & -5.21(4)  & 2.0(3)   & 0.0155(4)\\
            \bottomrule
        \end{tabular}
    }
\end{table}

\section{MD of QM9}
\label{sec:qm9_isomers_benchmark}
\TODO{Notes on training}

\section{QM9 - full?}
\label{sec:qm9_isomers_benchmark}
\TODO{Notes on training}
