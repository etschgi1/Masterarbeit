\chapter{A GNN approach to the problem}
\label{chap:gnn}
So far we have experimented with various models trying to predict the Fock matrix as a whole. However, as has been seen in \autoref{sec:further_trials} this approach, which works on minimal basis sets, is not feasible for larger basis sets such as the 6-31G basis set. Recent advances using regression models have largely focused on separate models for each molecular species, which limits their applicability to constitutional isomers we are interested in. \parencite{ref:Hazra2024,ref:Shao2023}
We shall investigate the applicability of Graph Neural Networks (GNNs) to the problem of predicting the Fock or the density matrix. They have shown promising results in the field of quantum chemistry, especially for predicting molecular properties and structures. \parencite{ref:schnet2018} \TODO{maybe other citation + intro round up} \\

The implementation of the GNN in this chapter is based on the \texttt{pytorch-geometric} framework \parencite{ref:PyTorchGeometric, ref:PyTorch_geom_paper}. 
\section{Input \& Output Matrices}
\TODO{Finish description of input / output matrices}
\label{sec:gnn_input_output_matrices}
Generally the input and the output of neural networks is fixed. We thus need to find a specific way to embed our input (overlap matrix) and output (Fock or density matrix) into a fixed structure. Given the nature of our problem, we can simply split our matrix representation into blocks representing the different atom sorts and their respective combinations. This yields three different types of blocks: 
\begin{itemize}
    \item \textbf{Self-blocks:} These blocks contain only orbitals of the same atom type, e.g. $\text{\ch{O}}_0$ or $\text{\ch{H}}_1$. The self-blocks are diagonal blocks of the input and output matrices.
    \item \textbf{Homo-blocks:} These blocks contain only orbitals of the same atom type, e.g. $\text{\ch{H}}_1 - \text{\ch{H}}_2$.
    \item \textbf{Hetero-blocks:} These blocks contain orbital combinations of different atom sorts, e.g. $\text{\ch{O}}_0 - \text{\ch{H}}_2$.
\end{itemize}
Subsequently, we shall label self-blocks as node-blocks and homo- / hetero-blocks as interaction blocks interchangeably. 

A schematic depiction is for the \ch{H2O} molecule using the 6-31G Basis is given in \autoref{fig:schematic_gnn_blocks}. Due to the symmetry of our input and output matrices we are only concerned with the upper triangular matrices for self-blocks and the homo- / hetero-blocks in the upper triangular part with respect to the full matrix. For the given example we thus have 351 entries for \ch{O} and 15 for \ch{H} self-blocks. Additionally, the \ch{H} homo-block yields 25 entries and we have two \ch{O}-\ch{H} hetero-blocks with 130 entries each. In total 666 entries suffice to reconstruct the full $36 \times 36$ matrix. 
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{../fig/gnn/schematic_blocks.pdf}
    \caption[Matrix block regions of \ch{H2O}]{Converged density of \ch{H2O} with matrix block types indicated.}
    \label{fig:schematic_gnn_blocks}
\end{figure}
Evidently, the different block types vary widely in size which needs to be addressed before feeding the data into the network. There are multiple ways of addressing this problem. The straightforward solution includes a simple zero pad to the maximum block size and subsequent masking. Other approaches could be a compresion or transformation to a fixed size latent space or some sort of pooling to reach the desired input dimension. To guarantee to not loose relevant data we take yet another route and encode each block using seperate encoders. Via this approach we mitigate the problem of different sized inputs as long as we stay with the same basis set. For the example in \autoref{fig:schematic_gnn_blocks} we would thus have four differently sized encoders (two for the self-blocks and one for homo-/ and hetero-blocks respectively). These encoders will transfer the input features into a common hidden dimension to be used in the network. After the propagation through our GNN the decoder side will decode the output dimension of our network to the respective block dimensions from which the whole matrix can be reconstructed.
\section{Preprocessing}
\label{sec:gnn_preproc}
Before feeding our data into the network we must bring it into a suitable form learning with the \texttt{pytorch-geometric} framework \parencite{ref:PyTorchGeometric, ref:PyTorch_geom_paper}. After introducing the design of our data objects in we discuss a way to tackle the problem's sensitivity to spatial rotation before closing out with normalization and dataset creation remarks. 
\subsection{Graph creation}
\label{subsec:gnn_graph_creation}
A graph representation of our data is required which will be populated using the paradigms described in \autoref{sec:gnn_input_output_matrices}. Nodes of our graph are essentially represented by embeddings of our overlap self-blocks. This leaves us with one node per atom in our molecule. To enable easier benchmarking and training processes the targets (Fock- or density matrix-self blocks) are also stored in our graph object. Nodes are connected by directed edges which are given by an embedding of the homo- / hetero-block regions under a given restriction. The restriction on edge formation is usually given by a simple distance cutoff (e.g. \SI{3}{\angstrom}), but not necesarily limited to that method. In particular, for bonds involving high-angular-momentum orbitals, the highly directed radial extension of the electronic wavefunction should also lead to an edge between these respective nodes. Edge formation can be controlled by a threshold with regard to the maximum value or another metric e.g. a matrix norm for a given interaction block. A tensor of indices is used to represent the directed edges between nodes identified with said index.\\

Additionally, to the node and edge embeddings of our inputs and targets we store the respective node atom-symbols and edge-symbols (e.g. \ch{C}-\ch{O}) to enable encoder / decoder matching in our network. Furthermore, we store global index information for both node and interaction blocks to facilitate matrix reconstruction from our blocks. 

\subsection{Data Augmentation}
\label{subsec:gnn_data_augmentation}
Our input (overlap) as well as our output (Fock / density matrix) are quantities which are not generally invariant under rotations of our molecular system. In essence this means that we must find a way to learn differently rotated molecules and produce their respective Fock / density matrices to later deduce our initial density. While initially the idea of making the input invariant under rotation by using a predetermined standard orientation to learn the problem was considered, problems arise with this approach. Most prominently, defining a standard orientation for isomers / isomer-parts (such as \ch{C7H10O2} or submolecules thereof) is far from trivial. Even if such a standard orientation is defined, one has to consider an additional pre- and post-processing step to rotate the input into the standard orientation and the output back to the original orientation. \\
Contrary to this, the model can learn differently rotated inputs to generate the corresponding outputs. This is achieved by augmenting the dataset with different rotations of the same molecules / submolecules. Rotating the input coordinates using a rotation matrix $R$ is in principle rather straightforward. However, the corresponding overlap, density and Fock matrices also have to be transformed accordingly or recalculated. The later is computationally not feasible, hence we use the corresponding Wigner D-matrices to transform input and output matrices. 
The Wigner D-matrix is a unitary matrix with $2L + 1$ rows and columns, where $L$ is the angular momentum. For a given rotation $R$ the Matrix elements of overlap, density and Fock matrix transform as follows:
\begin{equation}
    O'_{ij} = \sum_{k,l} \mathcal{D}^{(L)}_{ik}(R) O_{kl} \mathcal{D}^{(L)*}_{lj}(R)
\end{equation}
Naturally, the transformation only acts on spatial orbitals with no rotational symmetry along the axis of rotation (i.e. $L \neq 0$). Given our blocks defined in \autoref{sec:gnn_input_output_matrices} $\mathcal{D}^{(L)}$ will only transform elements of our blocks with at least one orbital having $L \neq 0$. \\


Practically, the data is augmented by sampling a random rotation axis (by sampling 3 normally distributed values and normalizing the given axis) and a random rotation angle $\theta \in [0, 2\pi]$. Given this axis and angle, the corresponding transformations are performed to the overlap, density and Fock matrices to obtain our augmented samples. New graphs are created using these transformed matrices as explained in the precious section and added to the dataset. \\
Due to the grid spacing in DFT calculations small deviations ($\approx 0,1 \unit{\milli\hartree}$ in the Fock matrix) between the transformed matrices and newley calculated ones using the rotated atom coordinates occur. These deviations only slightly affect the accuracy of the reconstructed density (see \autoref{eq:density_reconstruction_from_fock}) and are not relevant during training, as we evaluate the model against the provided target. In other words, if the model predicts the density via the Fock matrix, we compare it to a reference density that was also reconstructed from the (transformed) Fock, not from a directly transformed density matrix.\\
%! Note that translating the molecule should change absolutely nothing about the Overlap or Fock matrix. 

\subsection{Normalization \& data split}
\label{subsec:gnn_normalization}
Means and standard deviations are calculated on a per block-type basis (e.g. for the \ch{O}-\ch{H} hetero-block) on all training data samples. This is done for the input (overlap matrix) and the output / targets (Fock or density matrix).\\
It should be noted that the inclusion of data augmentation samples does not change these values besides machine percision errors. The normalization is then performed by subtracting the mean and dividing by the standard deviation for each block-type. To avoid numerical instabilities, especially for very sparse blocks (std. dev $\rightarrow 0$), a value of $10^{-6}$ is used should the standard deviation fall bellow this value for a given block. \\

The dataset is ultimately split into training, validation, and test sets, typically following a ratio of 80\% / 10\% / 10\%, where 100\% refers to the number of original (non-augmented) samples.

When applying a data augmentation factor of $1.5$, meaning that 50\% more samples are generated synthetically, the training set is extended to contain 120\% of the original data, while the validation and test sets remain unchanged at 10\% each. This ensures that evaluation is performed exclusively on the original, unaltered data and avoids any data leakage through augmented samples.

Such a split preserves the integrity of the validation and test phases and allows for a fair assessment of generalization performance.

\section{GNN Design}
\label{sec:gnn_design}


\TODO{Nice representation of orbitals on mols - maybe PyMol plot + some overlap matrix blocks}
\section{Training}
\label{sec:gnn_training}
\TODO{better text}
We initially train a model without data augmentation and only on a small subset of our QM9 C7H10O2 molecules (500 atoms simulated with B3LYPG \& pcseg-1) with a 80/10/10 split (train/val/test). 
Meta: 
\begin{verbatim}
batch_size=16,
hidden_dim=256,
train_val_test_ratio=(0.8, 0.1, 0.1),
message_passing_steps=3,
edge_threshold_val=5,
message_net_layers=3,
message_net_dropout=0.1,
target="fock",
lr=1e-3 
weight_decay=1e-5
trained for 103 epochs (overfitting started earlier - 
see history data in scripts>gnn>plot_data)
test_loss: 6.00
using standards from c2ec04be68ae09bec80153fabaace85234ad1a5e elsewhere!
\end{verbatim}
        
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{../fig/gnn/mgnn_pcseg1_simple_loss.pdf}
    \caption[GNN initial training Loss]{\TODO{...}}
    \label{fig:gnn_initial_training_loss}
\end{figure}